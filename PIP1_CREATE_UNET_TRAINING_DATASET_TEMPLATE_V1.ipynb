{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ra206/miniconda3/envs/unetgpu/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['plt']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import seaborn as sns\n",
    "import glob, os\n",
    "abra=os.getcwd()\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_and_isolate_training_data(result_files,subsetids=[0],new_folder_name=\"subsetted_training_data/{}_\"):\n",
    "    chosen_training_data=subsetids\n",
    "    for i in chosen_training_data:\n",
    "        begin=result_files[i].find('session')\n",
    "        strain_idx=chosen_training_data.index(i)\n",
    "        filename=new_folder_name.format(result_files[i][begin:begin+22]) + \"results_{:03d}.h5\".format(strain_idx)\n",
    "        print(filename)\n",
    "        copyfile(result_files[i],filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've used moseq to extract data and have a bunch of sessions \n",
    "# in a folder named {training_data_folder_name} that you would like to use to create a traing data set..\n",
    "\n",
    "training_data_folder_name = \"/n/groups/datta/Dana/longtogeny39/5_weeks/\"\n",
    "\n",
    "# assuming the folder contains all the data and you want to create a subset of that data {subsetids} to train with\n",
    "# which will go into a folder called {training_data_subset_folder_name}\n",
    "\n",
    "subsetids=[0,1]\n",
    "training_data_subset_folder_name='/n/groups/datta/Dana/longtogeny39/training_set_1/'\n",
    "os.mkdir(training_data_subset_folder_name)\n",
    "\n",
    "# finally you aggregate all the training sessions into one h5 file called {final_training_data_name}\n",
    "\n",
    "final_training_data_name='/n/groups/datta/Dana/longtogeny39/training_set_1.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703110809/proc/results_00.h5\n",
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703150932/proc/results_00.h5\n",
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703175607/proc/results_00.h5\n",
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703151004/proc/results_00.h5\n",
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703110659/proc/results_00.h5\n",
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703175537/proc/results_00.h5\n",
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703110739/proc/results_00.h5\n",
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703175558/proc/results_00.h5\n",
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703202215/proc/results_00.h5\n",
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703150953/proc/results_00.h5\n",
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703150943/proc/results_00.h5\n",
      "/n/groups/datta/Dana/longtogeny39/5_weeks/session_20190703110841/proc/results_00.h5\n"
     ]
    }
   ],
   "source": [
    "result_files=[] #create an empty matrix that will contain the addresses of the result files\n",
    "for file in glob.glob(training_data_folder_name+\"session*/proc/*.h5\"):\n",
    "    result_files.append(file)\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/groups/datta/Dana/longtogeny39/training_set_1/results_000.h5\n",
      "/n/groups/datta/Dana/longtogeny39/training_set_1/results_001.h5\n"
     ]
    }
   ],
   "source": [
    "subset_and_isolate_training_data(result_files,subsetids=subsetids,new_folder_name=training_data_subset_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/groups/datta/Dana/longtogeny39/training_set_1/results_001.h5\n",
      "/n/groups/datta/Dana/longtogeny39/training_set_1/results_000.h5\n"
     ]
    }
   ],
   "source": [
    "result_files=[] #create an empty matrix that will contain the addresses of the result files\n",
    "for file in glob.glob(training_data_subset_folder_name+\"*.h5\"):\n",
    "    result_files.append(file)\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames 1\n",
      "mask 1\n",
      "final shape of training frames (71925, 80, 80)\n"
     ]
    }
   ],
   "source": [
    "#append all the data together into one matrix\n",
    "\n",
    "#start with first file\n",
    "data=h5py.File(result_files[0], 'r+')\n",
    "frames=data['frames']\n",
    "mask=data['frames_mask']\n",
    "\n",
    "for i in range(len(result_files)):\n",
    "    if i !=0:\n",
    "        data=h5py.File(result_files[i], 'r+')\n",
    "        next_frames=data['frames']\n",
    "        next_mask=data['frames_mask']\n",
    "        #print('start')\n",
    "        frames=np.append(frames,next_frames,axis=0)\n",
    "        print(\"frames \" + str(i))\n",
    "        mask=np.append(mask,next_mask,axis=0)\n",
    "        print(\"mask \" + str(i))\n",
    "        \n",
    "print('final shape of training frames ' + str(frames.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and output the final h5 file\n",
    "\n",
    "data.close()\n",
    "\n",
    "#put new aggregated frames and mask into a file final file\n",
    "\n",
    "f = h5py.File(final_training_data_name, \"w\")\n",
    "dset = f.create_dataset('frames', data=frames)\n",
    "dset = f.create_dataset('frames_mask', data=mask)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shutil.rmtree('/n/groups/datta/rockwell/k2_data_gen2/subsetted_training_data/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
